# Projektstruktur & Skript-√úbersicht (`src/README.md`)

**Ziel:** Dieser Ordner enth√§lt alle Skripte zur Orchestrierung, Abfrage, Verarbeitung und **Klassifikation** der Bundestags-DIP-Daten. Die Pipeline ist auf die Analyse der **Dokumenten-Titel** fokussiert.

---

## üß© Skriptbeschreibung

| Datei | Funktion | Details |
|-------|-----------|----------------------------------------------------|
| **main.py** | **Pipeline-Orchestrierung** | Steuert den gesamten Prozess: Initialisiert Logging, ruft nacheinander `fetch_data`, `process_data` und `analyze_terms` auf. |
| **fetch_data.py** | **Datenabruf** | Holt Dokumente √ºber die DIP-API. **Ausschlie√ülich** der Endpunkt `/drucksache` f√ºr den Zeitraum 2019‚Äì2025 wird verwendet. |
| **process_data.py** | **Datenvorbereitung** | Bereinigt die Rohdaten. **Extrahiert nur ID, Titel, Datum** aus den `/drucksache`-JSONs und speichert sie als `data/processed/processed_documents.csv`. |
| **analyze_terms.py** | **Klassifikation** | F√ºhrt die zentrale Analyse durch: **Klassifiziert jeden Titel** in `eco_hit`, `sec_hit`, `mixed`, oder `none` und aggregiert die Z√§hler pro Jahr in `data/results/`. |
| **utils.py** | **Hilfsfunktionen** | Stellt Logging, JSON- und CSV-Handling bereit. |

---

## üß† Ablauf (Pipeline-Logik)

Die Pipeline wird √ºber **`main.py`** gestartet und f√ºhrt folgende Schritte aus:

1.  **Datenbeschaffung:** `fetch_data.py` ruft die Rohdaten ab (`data/raw/`).
2.  **Datenreinigung:** `process_data.py` erstellt die minimale Analyse-CSV (`data/processed/`).
3.  **Analyse:** `analyze_terms.py` klassifiziert die Daten und speichert die finalen Zeitreihen-Z√§hler (`data/results/`).

---

## üöÄ Nutzung

Um die vollst√§ndige Analyse durchzuf√ºhren, f√ºhren Sie das Hauptskript im `src/` Verzeichnis aus:

```bash
python main.py
