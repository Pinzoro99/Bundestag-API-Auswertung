# Projektstruktur & Skript-Ãœbersicht (`src/README.md`)

**Ziel:**  
Dieser Ordner enthÃ¤lt alle Skripte zur Abfrage, Verarbeitung und Analyse der Bundestags-DIP-Daten.  
Die Skripte basieren auf den Spezifikationen in `/specs/` und nutzen die Einstellungen aus `/config/settings.json`.

---

## ðŸ“‚ Empfohlene Ordnerstruktur

```text
project-root/
 â”œâ”€ config/
 â”‚   â””â”€ settings.json
 â”‚
 â”œâ”€ data/
 â”‚   â”œâ”€ raw/            # Original-JSON-Dateien aus der API (eine Datei pro Jahr oder pro API-Request)
 â”‚   â”œâ”€ processed/      # Bereinigte und kombinierte Textdaten
 â”‚   â”œâ”€ results/        # Ergebnisdaten (Keyword-Counts, Zeitreihen, etc.)
 â”‚   â””â”€ term_list.json  # Keyword-Cluster (Ã–kologie/Nachhaltigkeit vs. Sicherheit/Resilienz)
 â”‚
 â”œâ”€ logs/
 â”‚   â””â”€ api_fetch.log   # Log-Dateien der API-Abfragen
 â”‚
 â”œâ”€ specs/
 â”‚   â”œâ”€ 01_data_source.md
 â”‚   â”œâ”€ 02_query_design.md
 â”‚   â””â”€ 03_field_mapping.md
 â”‚
 â”œâ”€ src/
 â”‚   â”œâ”€ README.md        # (dieses Dokument)
 â”‚   â”œâ”€ fetch_data.py    # Holt Daten aus der Bundestags-DIP-API und speichert sie unter /data/raw/
 â”‚   â”œâ”€ process_data.py  # Kombiniert / bereinigt Daten laut field_mapping.md
 â”‚   â”œâ”€ analyze_terms.py # FÃ¼hrt Keyword-Frequenz- und Zeitreihenanalysen durch
 â”‚   â””â”€ utils.py         # Hilfsfunktionen (Logging, File Handling, Text Cleaning, etc.)
 â”‚
 â””â”€ .env.example         # Beinhaltet: DIP_API_KEY=DEIN_KEY
```

---

## ðŸ§© Skriptbeschreibung

| Datei | Funktion |
|-------|-----------|
| **fetch_data.py** | FÃ¼hrt die API-Requests aus, nutzt die Parameter aus `settings.json`, speichert Antworten in `data/raw/`. |
| **process_data.py** | FÃ¼hrt die beiden Endpunkte (`/drucksache` und `/vorgang`) zusammen, bereinigt Felder, erstellt kombinierte Textfelder. |
| **analyze_terms.py** | LÃ¤dt die verarbeiteten Texte und zÃ¤hlt die Keywords aus `data/term_list.json` pro Jahr und Cluster. Ergebnisse werden in `data/results/` gespeichert. |
| **utils.py** | EnthÃ¤lt gemeinsame Funktionen (z. B. Logging, Datumsparser, Textnormalisierung, JSON-Handling). |

---

## ðŸ§  Ablauf (Pipeline-Logik)

1. **fetch_data.py** â†’ ruft Daten ab und speichert Rohdaten.  
2. **process_data.py** â†’ bereitet sie auf und kombiniert relevante Textfelder.  
3. **analyze_terms.py** â†’ zÃ¤hlt Keywords, erstellt Zeitreihen und VerhÃ¤ltnisindikatoren.  
4. Ergebnisse â†’ in `data/results/` gespeichert, optional Visualisierung.

---

## ðŸ”§ ToDo / NÃ¤chste Schritte

- Erstellung der Skripte (Codex oder manuell)
- Implementierung der Keyword-ZÃ¤hlung und Zeitreihenaggregation
- Erweiterung der Cluster in `data/term_list.json` bei Bedarf

---

**Hinweis:**  
Die Skripte sollen modular aufgebaut werden â€“ d. h. jede Komponente (`fetch`, `process`, `analyze`) kann unabhÃ¤ngig ausgefÃ¼hrt werden.  
Alle Pfade und Parameter werden dynamisch aus `config/settings.json` gelesen.
